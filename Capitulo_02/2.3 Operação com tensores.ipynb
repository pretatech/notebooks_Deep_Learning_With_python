{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python\n",
    "\n",
    "2.3 The gears of neural networks: tensor operations\n",
    "\n",
    "Aqui vamos observar algumas opera√ß√µes com tensores, como opera√ß√µes elementares, broadcasting, dot, reshaping (muuuuito utilizado!!) e um pouco de geometria. \n",
    "Oh! Sem desistir, bora bora ! üí™üèæüí™üèæ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Element-wise (opera√ß√µes elementares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A opera√ß√£o elemento-a-elemento √© atuar em cada elemento do tensor separadamente. Por exemplo, implementamos um relu simples (relu (x) = max (x, 0)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_relu(x):\n",
    "    assert len(x.shape) == 2    # x √© um tensor 2D Numpy.\n",
    "    x = x.copy()    # Evite sobrescrever o tensor de entrada.\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A adi√ß√£o tamb√©m √© uma opera√ß√£o baseada no elemento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add(x, y):\n",
    "    # assert x and y are 2D Numpy tensors and have the same shape.\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    \n",
    "    x = x.copy()    # Avoid overwriting the input tensor.\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em Numpy, tudo isso √© escrito. O c√°lculo espec√≠fico √© realizado pelo BLAS escrito em C ou Fortran, e a velocidade √© muito alta.\n",
    "\n",
    "\n",
    "Voc√™ pode verificar se o BLAS est√° instalado assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blas_mkl_info:\n",
      "    libraries = ['mkl_rt']\n",
      "    library_dirs = ['C:/Users/Lenovo/Anaconda3/envs/tf\\\\Library\\\\lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['C:/Users/Lenovo/Anaconda3/envs/tf\\\\Library\\\\include']\n",
      "blas_opt_info:\n",
      "    libraries = ['mkl_rt']\n",
      "    library_dirs = ['C:/Users/Lenovo/Anaconda3/envs/tf\\\\Library\\\\lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['C:/Users/Lenovo/Anaconda3/envs/tf\\\\Library\\\\include']\n",
      "lapack_mkl_info:\n",
      "    libraries = ['mkl_rt']\n",
      "    library_dirs = ['C:/Users/Lenovo/Anaconda3/envs/tf\\\\Library\\\\lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['C:/Users/Lenovo/Anaconda3/envs/tf\\\\Library\\\\include']\n",
      "lapack_opt_info:\n",
      "    libraries = ['mkl_rt']\n",
      "    library_dirs = ['C:/Users/Lenovo/Anaconda3/envs/tf\\\\Library\\\\lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['C:/Users/Lenovo/Anaconda3/envs/tf\\\\Library\\\\include']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui est√° como usar relu element-wise do numpy, adicione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  9 11]\n",
      " [-3 -1 -2]\n",
      " [ 4 -1  8]]\n",
      "[[ 7  9 11]\n",
      " [ 0  0  0]\n",
      " [ 4  0  8]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = np.array([[1, 2, 3],\n",
    "              [-1, 2, -3],\n",
    "              [3, -1, 4]])\n",
    "b = np.array([[6, 7, 8], \n",
    "              [-2, -3, 1], \n",
    "              [1, 0, 4]])\n",
    "\n",
    "c = a + b    # Element-wise addition\n",
    "d = np.maximum(c, 0)    # Element-wise relu\n",
    "\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting (Transmiss√£o)\n",
    "\n",
    "Ao realizar opera√ß√µes com elementos, se as formas dos dois tensores forem diferentes, o tensor menor ser√° \"transmitido\" na mesma forma que o tensor maior, quando vi√°vel.\n",
    "\n",
    "Especificamente, voc√™ pode transmitir para duas folhas de forma (a, b, ..., n, n + 1, ..., m) e (n, n + 1, ..., m) Quantidades realizam opera√ß√µes elementares.\n",
    "\n",
    "Tal como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((64, 3, 32, 10))    # x is a random tensor with shape (64, 3, 32, 10).\n",
    "y = np.random.random((32, 10))    # y is a random tensor with shape (32, 10).\n",
    "z = np.maximum(x, y)    # The output z has shape (64, 3, 32, 10) like x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A opera√ß√£o de transmiss√£o √© a seguinte:\n",
    "\n",
    "1.Eixo de aumento de tensor pequeno (eixo de transmiss√£o), adicionar ao mesmo que o grande (ndim)\n",
    "\n",
    "\n",
    "2.Os elementos do tensor pequeno s√£o repetidos no novo eixo e adicionados √† mesma forma que o grande (forma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√â claro que na implementa√ß√£o real, n√£o copiamos dessa forma, o que √© um desperd√≠cio de espa√ßo, implementamos essa \"c√≥pia\" diretamente no algoritmo. Por exemplo, implementamos um vetor simples e adi√ß√£o de matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,   1, 103],\n",
       "       [  5,   4, 106],\n",
       "       [  8,   7, 109]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_add_matrix_and_vector(m, v):\n",
    "    assert len(m.shape) == 2    # m is a 2D Numpy tensor.\n",
    "    assert len(v.shape) == 1    # v is a Numpy vector.\n",
    "    assert m.shape[1] == v.shape[0]\n",
    "    \n",
    "    m = m.copy()\n",
    "    for i in range(m.shape[0]):\n",
    "        for j in range(m.shape[1]):\n",
    "            m[i, j] += v[j]\n",
    "    return m\n",
    "\n",
    "naive_add_matrix_and_vector(np.array([[1 ,2, 3], [4, 5, 6], [7, 8, 9]]), \n",
    "                            np.array([1, -1, 100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Dot\n",
    "Produto escalar tensor (ponto)\n",
    "O produto escalar do tensor, ou produto do tensor, √© feito com o ponto (x, y) em numpy.\n",
    "\n",
    "A opera√ß√£o do produto escalar pode ser vista a partir do seguinte programa simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produto escalar vetorial\n",
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    \n",
    "    z = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z\n",
    "\n",
    "\n",
    "# Matriz e produto escalar vetorial\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z\n",
    "\n",
    "\n",
    "# Produto escalar matricial\n",
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.,   1.,  22.],\n",
       "       [-13., -13., -18.],\n",
       "       [ 24.,  24.,  39.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3],\n",
    "              [-1, 2, -3],\n",
    "              [3, -1, 4]])\n",
    "b = np.array([[6, 7, 8], \n",
    "              [-2, -3, 1], \n",
    "              [1, 0, 4]])\n",
    "naive_matrix_dot(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mesmo √© verdadeiro para produtos de ponto tensor de alta dimens√£o. Por exemplo, (trata-se de forma):\n",
    "\n",
    "(a, b, c, d) . (d,) -> (a, b, c)\n",
    "\n",
    "(a, b, c, d) . (d, e) -> (a, b, c, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping\n",
    "Deforma√ß√£o do tensor (remodelagem)\n",
    "\n",
    "\n",
    "Essa opera√ß√£o, em suma, ainda s√£o esses elementos, mas a forma como est√£o dispostos mudou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0., 1.],\n",
    "              [2., 3.],\n",
    "              [4., 5.]])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [5.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape((6, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [3., 4., 5.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape((2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Transposi√ß√£o\" (transposi√ß√£o) √© uma transforma√ß√£o especial de matriz. Transposi√ß√£o √© a troca de linhas e colunas.\n",
    "\n",
    "\n",
    "O original x [i,:], ap√≥s a transposi√ß√£o, torna-se x [:, i]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300)\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((300, 20))\n",
    "y = np.transpose(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ufa!! Passou! Sobrevivemos? Ent√£o vamos logo para o pr√≥ximo cap√≠tulo. Vem vem. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
