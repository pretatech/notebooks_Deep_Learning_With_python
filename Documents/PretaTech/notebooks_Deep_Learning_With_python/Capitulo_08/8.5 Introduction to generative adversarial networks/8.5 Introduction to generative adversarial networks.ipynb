{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de mais nada os notebooks aqui mostrado tiveram como base/foram retirados dos seguintes reposit√≥rios: \n",
    " > https://github.com/fchollet/deep-learning-with-python-notebooks \n",
    " \n",
    " \n",
    " > https://github.com/cdfmlr/Deep-Learning-with-Python-Notebooks\n",
    " \n",
    " Sugiro fortemente que consultem os c√≥digos originais e em caso de d√∫vida podem me contatar para conversarmos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado profundo com Python\n",
    "\n",
    "## 8.5 Introdu√ß√£o √†s redes advers√°rias geradoras\n",
    "\n",
    "> Introdu√ß√£o √†s Redes Adversariais Gerativas\n",
    "\n",
    "GAN, chin√™s √© ~~ gan ~~, errado, Generative Adversarial Network. Como VAE, √© usado para aprender o espa√ßo latente da imagem. Isso pode tornar a imagem gerada \"estatisticamente\" igual √† imagem real, ou seja, a imagem gerada √© bastante realista. No entanto, ao contr√°rio do VAE, o espa√ßo latente de GAN n√£o pode ser garantido como tendo uma estrutura significativa e n√£o √© cont√≠nuo.\n",
    "\n",
    "GAN consiste em duas partes:\n",
    "\n",
    "-Rede geradora: insira um vetor aleat√≥rio (um ponto aleat√≥rio no espa√ßo latente) e decodifique-o em uma imagem.\n",
    "- Rede discriminadora: Insira uma imagem (real ou desenhada pelo gerador) e preveja se a imagem √© real ou criada pela rede do gerador. (A rede discriminadora tamb√©m √© chamada de \"advers√°rio\", advers√°rio)\n",
    "\n",
    "O objetivo do treinamento do GAN √© permitir que a \"rede geradora\" engane a \"rede discriminadora\".\n",
    "\n",
    "A compreens√£o intuitiva de GAN √© uma hist√≥ria muito inspiradora: ou seja, h√° duas pessoas, um falsificador e um avaliador. Os falsificadores imitam as pinturas do mestre e, em seguida, misturam suas imita√ß√µes com as originais e as submetem ao avaliador para avalia√ß√£o, que avalia a autenticidade de cada pintura e v√™ atrav√©s de quais s√£o forjadas. O am√°vel avaliador deu feedback ao falsificador e disse-lhe as caracter√≠sticas do original. Os falsificadores ir√£o, passo a passo, melhorar sua capacidade de falsifica√ß√£o com base nas opini√µes do avaliador. Os dois repetiram esse processo incansavelmente: os falsificadores tornaram-se cada vez mais bons em copiar pinturas de mestres e os avaliadores se tornaram cada vez mais bons em encontrar pinturas falsas. No final, os falsificadores criaram um lote de \"produtos de imita√ß√£o\" que os avaliadores imp√µem.\n",
    "\n",
    "\n",
    "O m√©todo de treinamento do GAN √© muito especial e seu m√≠nimo otimizado n√£o √© fixo. Nossa \"descida gradiente\" usual √© rolar montanha abaixo ao longo do terreno de perda est√°tica, mas cada degrau da montanha durante o treinamento GAN mudar√° todo o terreno. √â um sistema din√¢mico e seu processo de otimiza√ß√£o √© o equil√≠brio entre duas for√ßas. Portanto, GAN √© dif√≠cil de treinar. Para fazer o GAN funcionar normalmente, √© necess√°rio muito trabalho de constru√ß√£o de modelo e ajuste de hiperpar√¢metro.\n",
    "\n",
    "### Rede Adversarial Gerativa Convolucional Profunda\n",
    "\n",
    "Vamos tentar implementar o GAN mais simples e simples com Keras. Especificamente, construiremos uma ** rede convolucional profunda de confronto gerador ** (GAN convolucional profunda, DCGAN). O gerador e o discriminador desse tipo de coisa s√£o ambas redes neurais convolucionais profundas.\n",
    "\n",
    "Vamos treinar DCGAN com imagens da categoria \"sapo\" no conjunto de dados CIFAR10. Este conjunto de dados cont√©m 50000 imagens RGB 32 √ó 32, que pertencem a 10 categorias (5000 imagens em cada categoria).\n",
    "\n",
    "#### Processo de implementa√ß√£o\n",
    "\n",
    "1. A rede `generator` mapeia um vetor de forma` (latent_dim,) `para uma imagem de forma` (32, 32, 3) `.\n",
    "2. A rede `discriminator` mapeia a imagem com a forma` (32, 32, 3) `para uma pontua√ß√£o bin√°ria, que √© usada para avaliar a probabilidade de a imagem ser verdadeira.\n",
    "3. A rede `gan` conecta` gerador` e `discriminador` juntos:` gan (x) = discriminador (gerador (x)) `. Esta rede mapeia o vetor latente para o resultado da avalia√ß√£o do discriminador.\n",
    "4. Use as amostras de imagens verdadeiras e falsas rotuladas com `\" real \"` ou `\" falsa \"` para treinar o discriminador e use o m√©todo usual de treinar modelos de classifica√ß√£o de imagem comuns.\n",
    "5. Para treinar o gerador, use o gradiente de perda do modelo `gan` em rela√ß√£o ao peso do gerador. A cada etapa, o peso do gerador deve ser movido para \"tornar o discriminador mais prov√°vel de classificar a imagem decodificada pelo gerador como\" verdadeira \"\", ou seja, treinar o gerador para enganar o discriminador.\n",
    "\n",
    "#### Habilidades pr√°ticas\n",
    "\n",
    "O processo de treinamento e ajuste do GAN √© muito dif√≠cil. Portanto, precisamos nos lembrar de algumas habilidades pr√°ticas resumidas por pessoas anteriores. Essas t√©cnicas geralmente s√£o √∫teis, mas n√£o se aplicam a todas as situa√ß√µes. Essas coisas n√£o t√™m base te√≥rica, s√£o todas metaf√≠sicas, ent√£o escreva a conclus√£o diretamente, sem explica√ß√£o:\n",
    "\n",
    "-Use tanh como a ativa√ß√£o da √∫ltima camada do gerador em vez de sigm√≥ide.\n",
    "-Use a distribui√ß√£o normal (distribui√ß√£o gaussiana) para amostrar pontos no espa√ßo latente em vez da distribui√ß√£o uniforme.\n",
    "-A aleatoriedade pode melhorar a robustez. O treinamento GAN pode ficar \"preso\" de v√°rias maneiras (atingindo o equil√≠brio din√¢mico errado). A introdu√ß√£o da aleatoriedade durante o processo de treinamento ajuda a prevenir essa situa√ß√£o. Existem duas maneiras de introduzir a aleatoriedade:\n",
    "    1. Use dropout no discriminador;\n",
    "    2. Adicione ru√≠do aleat√≥rio ao r√≥tulo do discriminador;\n",
    "- Gradiente esparso atrapalhar√° o treinamento GAN \"Opera√ß√£o m√°xima de pooling\" e \"ativa√ß√£o ReLU\" podem levar a gradientes esparsos, por isso √© recomendado:\n",
    "    1. Use `` convolu√ß√£o em etapas '' ao inv√©s de `` max pooling '' para reduzir a resolu√ß√£o;\n",
    "    2. Use a camada LeakyReLU para substituir a ativa√ß√£o ReLU;\n",
    "-Nas imagens geradas, artefatos quadriculados s√£o frequentemente vistos, o que se deve √† cobertura desigual do espa√ßo do pixel no gerador. A solu√ß√£o para este problema √©: sempre que o passo Conv2DTranpose ou Conv2D √© usado no gerador e discriminador, o tamanho do kernel deve ser divis√≠vel pelo passo.\n",
    "\n",
    "\n",
    "#### A realiza√ß√£o do gerador\n",
    "\n",
    "Comecei a construir o primeiro GAN para jovens! !\n",
    "\n",
    "Primeiro desenvolva o modelo do gerador: converta o vetor do espa√ßo latente em uma imagem candidata.\n",
    "\n",
    "Para evitar \"travar\" durante o treinamento, o dropout √© usado tanto no discriminador quanto no gerador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32768)             1081344   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# GAN Rede Geradora\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "\n",
    "latent_dim = 32\n",
    "\n",
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# Converta a entrada em um mapa de recursos de 128 canais com um tamanho de 16 √ó 16\n",
    "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16, 16, 128))(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# A amostragem de aumento √© de 32 √ó 32\n",
    "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Gere um mapa de recursos de tamanho 32 √ó 32 (o formato da imagem CIFAR10)\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "\n",
    "generator = keras.models.Model(generator_input, x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementa√ß√£o do discriminador\n",
    "\n",
    "Em seguida, desenvolva o modelo discriminador, insira uma imagem (real ou sint√©tica) e divida-a em \"verdadeiro\" (imagem real do conjunto de treinamento) ou \"falso\" (imagem desenhada pelo gerador)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# GAN Rede discriminadora\n",
    "\n",
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "discriminator = keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()\n",
    "\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(\n",
    "    lr=0.0008,\n",
    "    clipvalue=1.0,\n",
    "    decay=1e-8)\n",
    "\n",
    "discriminator.compile(optimizer=discriminator_optimizer,\n",
    "                      loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confrontando a Internet\n",
    "\n",
    "Finalmente, configure o GAN, conecte o gerador e o discriminador e converta os pontos no espa√ßo latente em um julgamento de classifica√ß√£o verdadeiro ou falso.\n",
    "\n",
    "Ao treinar este modelo, √© necess√°rio congelar o ‚Äúdiscriminador‚Äù (torn√°-lo intrat√°vel), e apenas permitir que o ‚Äúgerador‚Äù se mova na dire√ß√£o de ‚Äúmelhorar a habilidade do discriminador de engano‚Äù.\n",
    "\n",
    "Ao treinar gan, tudo o que usamos s√£o os r√≥tulos de \"imagens reais\", portanto, se o peso do \"discriminador\" puder ser atualizado durante o processo de treinamento, o treinamento far√° com que o discriminador sempre preveja \"verdadeiro\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rede de Confronto\n",
    "\n",
    "discriminator.trainable = False    # Isso s√≥ funcionar√° em gan\n",
    "\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "\n",
    "gan_optimizer = keras.optimizers.RMSprop(\n",
    "    lr=0.0004,\n",
    "    clipvalue=1.0,\n",
    "    decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer,\n",
    "            loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treine DCGAN\n",
    "\n",
    "O fluxo do loop de treinamento √© o seguinte:\n",
    "\n",
    "1. Extraia pontos aleat√≥rios (ru√≠do aleat√≥rio) do espa√ßo latente.\n",
    "2. D√™ este ru√≠do aleat√≥rio ao gerador para gerar uma imagem.\n",
    "3. Misture a imagem gerada com a imagem real.\n",
    "4. Use essas imagens misturadas e os r√≥tulos correspondentes (a imagem real √© \"verdadeira\" e a imagem gerada √© \"falsa\") para treinar o discriminador.\n",
    "5. Desenhe aleatoriamente novos pontos no espa√ßo latente.\n",
    "6. Use esses vetores e r√≥tulos aleat√≥rios que s√£o \"imagens reais\" para treinar gan.\n",
    "\n",
    "Implementa√ß√£o de c√≥digo espec√≠fico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: discriminator loss: 0.6944685578346252, adversarial loss: 0.7566524744033813\n",
      "step 50: discriminator loss: 0.6997207999229431, adversarial loss: 0.7283448576927185\n",
      "step 100: discriminator loss: 0.6917451024055481, adversarial loss: 0.7818207144737244\n",
      "step 150: discriminator loss: 0.7006672620773315, adversarial loss: 0.77472984790802\n",
      "step 200: discriminator loss: 0.6959202885627747, adversarial loss: 0.7540444731712341\n",
      "step 250: discriminator loss: 0.6907225847244263, adversarial loss: 0.7319836020469666\n",
      "step 300: discriminator loss: 0.6964272856712341, adversarial loss: 0.795426070690155\n",
      "step 350: discriminator loss: 0.7021819949150085, adversarial loss: 0.7412662506103516\n",
      "step 400: discriminator loss: 0.6954243183135986, adversarial loss: 0.7441832423210144\n",
      "step 450: discriminator loss: 0.7256754636764526, adversarial loss: 0.7400968074798584\n",
      "step 500: discriminator loss: 0.7020201683044434, adversarial loss: 0.7446410059928894\n",
      "step 550: discriminator loss: 0.6801480054855347, adversarial loss: 0.9143943786621094\n",
      "step 600: discriminator loss: 0.6830952763557434, adversarial loss: 0.7880680561065674\n",
      "step 650: discriminator loss: 0.7454708218574524, adversarial loss: 0.782561182975769\n",
      "step 700: discriminator loss: 0.6792200803756714, adversarial loss: 0.7939407825469971\n",
      "step 750: discriminator loss: 0.6916797757148743, adversarial loss: 0.8157812356948853\n",
      "step 800: discriminator loss: 0.6902390718460083, adversarial loss: 0.750106692314148\n",
      "step 850: discriminator loss: 0.6986071467399597, adversarial loss: 0.7653893232345581\n",
      "step 900: discriminator loss: 0.7069215774536133, adversarial loss: 0.7258030772209167\n",
      "step 950: discriminator loss: 0.6925474405288696, adversarial loss: 0.7341045141220093\n",
      "999/1000: 12.41s - ETA: 12ss\r"
     ]
    }
   ],
   "source": [
    "# GAN Treinamento,\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Importar dados CIFAR10\n",
    "(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n",
    "# Selecione a imagem do sapoüê∏\n",
    "x_train = x_train[y_train.flatten() == 6]\n",
    "\n",
    "x_train = x_train.reshape(\n",
    "    (x_train.shape[0],) + (height, width, channels)\n",
    ").astype('float32') / 255.\n",
    "\n",
    "\n",
    "iterations = 10000\n",
    "batch_size = 20\n",
    "save_dir = 'gan_save'\n",
    "\n",
    "start = 0\n",
    "for step in range(iterations+1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Amostra aleatoriamente de pontos potenciais\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    \n",
    "    # Gerar imagem\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "    \n",
    "    # Escolha uma imagem real\n",
    "    stop = start + batch_size\n",
    "    real_images = x_train[start: stop]\n",
    "    \n",
    "    # Combine imagens reais e geradas e d√™ r√≥tulos\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)),\n",
    "                             np.zeros((batch_size, 1))])\n",
    "    labels += 0.05 * np.random.random(labels.shape)  # Adicionar ru√≠do aleat√≥rio ao r√≥tulo\n",
    "    \n",
    "    # Discriminador de treinamento\n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "    \n",
    "    # Amostra aleatoriamente de pontos potenciais\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    \n",
    "    misleading_targets = np.zeros((batch_size, 1))  # Minta que todas s√£o fotos reais\n",
    "    \n",
    "    # Gerador de trem atrav√©s do modelo gan\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    start += batch_size\n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "        \n",
    "    if step % 50 == 0:\n",
    "        gan.save_weights('gan.h5')\n",
    "        \n",
    "        print(f'step {step}: discriminator loss: {d_loss}, adversarial loss: {a_loss}')\n",
    "        \n",
    "        img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, f'generated_frog_{step}.png'))\n",
    "        \n",
    "        img = image.array_to_img(real_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, f'real_frog_{step}.png'))\n",
    "    else:\n",
    "        time_cost = end_time - start_time\n",
    "        time_eta = time_cost * (iterations - step)\n",
    "        print(f'{step}/{iterations}: {time_cost:.2f}s - ETA: {time_eta:.0f}s', end='\\r')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: discriminator loss: 0.7096449136734009, adversarial loss: 0.752526581287384\n"
     ]
    }
   ],
   "source": [
    "for step in range(iterations, iterations+1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Amostra aleatoriamente de pontos potenciais\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    \n",
    "    # Gerar imagem\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "    \n",
    "    # Escolha uma imagem real\n",
    "    stop = start + batch_size\n",
    "    real_images = x_train[start: stop]\n",
    "    \n",
    "    # Combine imagens reais e geradas e d√™ r√≥tulos\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)),\n",
    "                             np.zeros((batch_size, 1))])\n",
    "    labels += 0.05 * np.random.random(labels.shape)  # Adicionar ru√≠do aleat√≥rio ao r√≥tulo\n",
    "    \n",
    "    # Discriminador de treinamento\n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "    \n",
    "    # Amostra aleatoriamente de pontos potenciais\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    \n",
    "    misleading_targets = np.zeros((batch_size, 1))  # Minta que todas s√£o fotos reais\n",
    "    \n",
    "    # Gerador de trem atrav√©s do modelo gan\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    start += batch_size\n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "        \n",
    "    if step % 50 == 0:\n",
    "        gan.save_weights('gan.h5')\n",
    "        \n",
    "        print(f'step {step}: discriminator loss: {d_loss}, adversarial loss: {a_loss}')\n",
    "        \n",
    "        img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, f'generated_frog_{step}.png'))\n",
    "        \n",
    "        img = image.array_to_img(real_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, f'real_frog_{step}.png'))\n",
    "    else:\n",
    "        time_cost = end_time - start_time\n",
    "        time_eta = time_cost * (iterations - step)\n",
    "        print(f'{step}/{iterations}: {time_cost:.2f}s - ETA: {time_eta:.0f}s', end='\\r')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A imagem de sa√≠da final:\n",
    "\n",
    "! [A √∫ltima imagem de sa√≠da] (https://tva1.sinaimg.cn/large/007S8ZIlgy1gi27n2ulptj300w00w0si.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
