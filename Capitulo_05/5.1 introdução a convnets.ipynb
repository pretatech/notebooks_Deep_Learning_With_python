{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python\n",
    "5.1 Introduction to convnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de mais nada os notebooks aqui mostrado tiveram como base/foram retirados dos seguintes repositórios: \n",
    " > https://github.com/fchollet/deep-learning-with-python-notebooks \n",
    " \n",
    " \n",
    " > https://github.com/cdfmlr/Deep-Learning-with-Python-Notebooks\n",
    " \n",
    " Sugiro fortemente que consultem os códigos originais e em caso de dúvida podem me contatar para conversarmos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introdução a redes neurais de courger\n",
    "\n",
    "Redes neurais de bobinas lidam muito bem com problemas de visão computacional.\n",
    "\n",
    "Vamos começar com um exemplo do mais simples processamento de rede neural courger MNIST completa capítulo ii da rede de conexão completa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui usamos a camada Conv2D para input_shape (image_height, image_width, image_channels) neste formato.\n",
    "\n",
    "A saída das camadas Conv2D e Max Pooling 2D é do tipo: (altura, largura, canais), na qual altura e largura são reduzidas camada por camada, e os canais são controlados pelo primeiro parâmetro de Conv2D.\n",
    "\n",
    "Nas últimas três camadas, transformamos as dezenas da última camada Conv2D (3, 3, 64) no vetor de características através da camada de Flatenning.  E logo depois as camadas densas com as funções de ativação que já vimos aqui. \n",
    "\n",
    "Por fim, veja a estrutura do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bem, a rede é construída assim, e então é treinada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 34s 564us/sample - loss: 0.1743 - accuracy: 0.9453\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 36s 605us/sample - loss: 0.0484 - accuracy: 0.9849\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 38s 636us/sample - loss: 0.0323 - accuracy: 0.9904\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 40s 672us/sample - loss: 0.0251 - accuracy: 0.9924\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 40s 670us/sample - loss: 0.0206 - accuracy: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2148035e348>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treine a rede neural do rolo na imagem MNIST\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dar uma olhada nos resultados no conjunto de testes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 2s - loss: 0.0276 - accuracy: 0.9916\n",
      "0.02759703517325979 0.9916\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes neurais de carretel\n",
    "\n",
    "A densa camada de conexão que usamos anteriormente aprendeu o padrão global em todo o espaço de recurso de entrada (todos os pixels no MNIST); Ou seja, Dense aprende toda a imagem, e Conv aprende a parte da imagem, como a janela 3x3 no código que acabamos de escrever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta rede neural cambaleante tem duas propriedades:\n",
    "\n",
    "O padrão aprendido pelas redes neurais converse é translacional: depois que a rede neural convutiva aprende um padrão e vê o mesmo padrão em outros lugares, reconhece que aprendeu isso e não precisa aprender novamente. E para a rede de Dense, mesmo que você encontre a mesma parte local, você ainda tem que reaprender. Esta propriedade permite que a rede neural do rolo use dados de forma eficiente, e requer menos amostras de treinamento para aprender uma melhor representação de dados de generalização (uma a uma, não por mapeamento como um todo).\n",
    "\n",
    "A rede neural do rolo pode aprender a hierarquia espacial dos padrões: depois que a rede neural do rolo aprendeu um pequeno padrão local na primeira camada, a próxima camada pode soletrar padrões maiores com esses pequenos locais. Então, mais algumas camadas, a rede neural do rolo pode aprender mais e mais complexos, mais e mais conceitos visuais abstratos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A camada de carretel\n",
    "\n",
    "\n",
    "As dezenas 3D de posições que usamos para representar a imagem no exemplo que acabamos de usar incluem dois eixos espaciais de altura, largura e uma profundidade de eixo de profundidade (também conhecido como eixo de canais), para imagens RGB, a dimensão do eixo de profundidade é 3, representando 3 cores, e para MNIST esta imagem em escala de cinza, a profundidade é 1, com apenas um número para representar o valor em escala de cinza. O resultado deste volume 3D e da operação de revolta feita nele é chamado de mapa de recursos.\n",
    "\n",
    "A operação do rolo extrai um pequeno pedaço do gráfico de recurso de entrada e impõe a mesma transformação em todos eles para obter o gráfico de recursos de saída. O gráfico do recurso de saída ainda é um campo 3D: com largura e altura, sua profundidade pode ser arbitrária, a profundidade é um argumento para a camada, e cada canal no eixo de profundidade representa um filtro (filtro). O filtro codifica um aspecto dos dados de entrada, por exemplo, um filtro pode codificar o conceito de \"um rosto está incluído na entrada\".\n",
    "\n",
    "No exemplo do MNIST há pouco, a primeira camada de carretel aceita um gráfico de recurso de entrada com dimensões (28, 28, 1) e produz um gráfico de características com dimensões (26, 26, 32). Esta saída contém 32 filtros, e o canal em cada eixo de profundidade contém um valor de 26x26, chamado mapa de resposta do filtro à entrada, que representa o resultado da operação do filtro em diferentes locais da entrada. É por isso que os gráficos de recursos são chamados de gráficos de recursos: cada dimensão do eixo de profundidade é um recurso (ou filtro), e a saída 2D outshow é um gráfico espacial bidimensional da resposta do filtro na entrada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
